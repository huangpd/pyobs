import os
import math
import time
import logging
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED
from obs import ObsClient, CompleteMultipartUploadRequest, CompletePart, ListMultipartUploadsRequest
from tqdm import tqdm
import http.client
from .exceptions import PartLimitExceededError, PartUploadError

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("ObsStream")

# ==========================================
# ğŸ©¹ çŒ´å­è¡¥ä¸ (Monkey Patch)
# ä¿®å¤ huaweicloud-sdk-python-obs SSL è¿æ¥å‚æ•°æŠ¥é”™é—®é¢˜
# ==========================================
def _patched_get_server_connection(self, is_secure, server, port, proxy_host, proxy_port):
    """è¦†ç›– SDK åŸæœ‰çš„è¿æ¥åˆ›å»ºæ–¹æ³•ï¼Œç§»é™¤ä¸æ”¯æŒçš„ check_hostname å‚æ•°"""
    if proxy_host is not None and proxy_port is not None:
        server = proxy_host
        port = proxy_port

    if is_secure:
        # å…³é”®ä¿®æ”¹ï¼šç›´æ¥ç§»é™¤ check_hostname å‚æ•°
        # Python 3 çš„ HTTPSConnection ä¼šè‡ªåŠ¨ä½¿ç”¨ context ä¸­çš„é…ç½®
        try:
            conn = http.client.HTTPSConnection(
                server,
                port=port,
                timeout=self.timeout,
                context=self.context
            )
        except TypeError:
            # å…œåº•ï¼šä¸‡ä¸€ context ä¹Ÿä¸æ”¯æŒï¼ˆæå°‘è§ï¼‰ï¼Œåˆ™ä¸ä¼  context
            conn = http.client.HTTPSConnection(
                server,
                port=port,
                timeout=self.timeout
            )
    else:
        conn = http.client.HTTPConnection(server, port=port, timeout=self.timeout)

    return conn

# åº”ç”¨è¡¥ä¸ï¼šæ›¿æ¢ ObsClient ç±»çš„å†…éƒ¨æ–¹æ³•
ObsClient._get_server_connection_use_http1x = _patched_get_server_connection
# ==========================================
# ğŸ©¹ çŒ´å­è¡¥ä¸ ç»“æŸ
# ==========================================

class UploadContext:
    """
    ä¸Šä¼ ä¸Šä¸‹æ–‡ï¼šç”¨äºåœ¨åˆå§‹åŒ–å’Œå®é™…ä¸Šä¼ ä¹‹é—´ä¼ é€’çŠ¶æ€
    """

    def __init__(self, object_key, upload_id, offset, next_part):
        self.key = object_key
        self.upload_id = upload_id  # OBS å†…éƒ¨ä»»åŠ¡ ID (ç”¨äºç»­ä¼ )
        self.offset = offset  # å·²ä¸Šä¼ å­—èŠ‚æ•° (ç”¨äºå‘Šè¯‰ä¸‹è½½å™¨ Range)
        self.next_part = next_part  # ä¸‹ä¸€ä¸ªåˆ†ç‰‡å· (ç”¨äºåˆ†ç‰‡æ’åº)


class StreamUploader:
    """
    åä¸ºäº‘ OBS æµå¼ä¸Šä¼ å·¥å…· (æ”¯æŒæ–­ç‚¹ç»­ä¼ )
    """

    # --- åˆ†ç‰‡ä¸Šä¼ å¸¸é‡ ---
    MAX_PARTS = 10000  # OBS åˆ†ç‰‡ä¸Šä¼ çš„ç¡¬æ€§ä¸Šé™
    SAFE_PARTS_COUNT = 8000  # å®‰å…¨åˆ†ç‰‡æ•°ï¼Œé¢„ç•™ 20% ç¼“å†²åº”å¯¹ Content-Length ä¸å‡†ç¡®
    SAFETY_THRESHOLD = 5000  # è°ƒæ•´ä¸º 5000ï¼Œææ—©ä»‹å…¥ï¼Œé¿å…åæœŸå†…å­˜æ¿€å¢
    MIN_PART_SIZE = 5 * 1024 * 1024  # 5MB (S3/OBS åè®®è¦æ±‚å‰ N-1 ä¸ªåˆ†ç‰‡ >= 5MB)

    def __init__(self, ak=None, sk=None, server=None, bucket_name=None,
                 part_size=20 * 1024 * 1024):
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼Œæ”¯æŒæ— å‚åˆå§‹åŒ–
        self.ak = ak or os.getenv("OBS_AK")
        self.sk = sk or os.getenv("OBS_SK")
        self.server = server or os.getenv("OBS_SERVER")
        self.bucket = bucket_name or os.getenv("OBS_BUCKET")

        if not all([self.ak, self.sk, self.server, self.bucket]):
            raise ValueError("å¿…é¡»æä¾› AK, SK, Server å’Œ Bucket Name (å¯é€šè¿‡å‚æ•°æˆ–ç¯å¢ƒå˜é‡)")

        self.client = ObsClient(access_key_id=self.ak, secret_access_key=self.sk, server=self.server)

        # --- æ ¸å¿ƒé…ç½® ---
        # åˆ†ç‰‡å¤§å°é»˜è®¤ 20MBï¼Œæ—¢èƒ½ä¿è¯å¹¶å‘åº¦ï¼Œåˆé€‚é…æµå¼åœºæ™¯
        self.part_size = part_size
        self.max_workers = 5  # å¹¶å‘ä¸Šä¼ çº¿ç¨‹æ•°
        self.max_retries = 5  # å•ä¸ªåˆ†ç‰‡ä¸Šä¼ å¤±è´¥é‡è¯•æ¬¡æ•°

    def abort_upload(self, object_key, upload_id=None):
        """
        å–æ¶ˆæŒ‡å®šçš„ä¸Šä¼ ä»»åŠ¡ï¼ˆæ¸…ç†å·²ä¸Šä¼ çš„åˆ†ç‰‡ï¼‰
        :param object_key: OBS ç›®æ ‡è·¯å¾„
        :param upload_id: (å¯é€‰) æŒ‡å®šçš„ upload_idï¼Œå¦‚æœä¸ä¼ åˆ™è‡ªåŠ¨æŸ¥æ‰¾
        :return: True æˆåŠŸå–æ¶ˆï¼ŒFalse æœªæ‰¾åˆ°ä»»åŠ¡
        """
        if not upload_id:
            # è‡ªåŠ¨æŸ¥æ‰¾è¯¥ key çš„ä¸Šä¼ ä»»åŠ¡
            upload_id, _, next_part = self._get_resume_info(object_key)
            if not upload_id:
                logger.info(f"æœªæ‰¾åˆ° {object_key} çš„æœªå®Œæˆä¸Šä¼ ä»»åŠ¡")
                return False
            logger.info(f"æ‰¾åˆ°ä»»åŠ¡: {upload_id}, å·²æœ‰åˆ†ç‰‡: {next_part - 1}")
        
        try:
            resp = self.client.abortMultipartUpload(self.bucket, object_key, upload_id)
            if resp.status < 300:
                logger.info(f"âœ… å·²æˆåŠŸå–æ¶ˆä¸Šä¼ ä»»åŠ¡: {object_key} (ID: {upload_id})")
                return True
            else:
                logger.error(f"å–æ¶ˆå¤±è´¥: {resp.errorMessage}")
                return False
        except Exception as e:
            logger.error(f"å–æ¶ˆä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

    def init_upload(self, object_key):
        """
        ã€ç¬¬ä¸€æ­¥ã€‘åˆå§‹åŒ–ä¸Šä¼ ä»»åŠ¡ï¼Œæ¢æµ‹æ–­ç‚¹
        :param object_key: OBS ç›®æ ‡è·¯å¾„
        :return: UploadContext å¯¹è±¡
        """
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–ä»»åŠ¡: {object_key} ...")

        # 1. å» OBS æŸ¥æœ‰æ²¡æœ‰æœªå®Œæˆçš„ä»»åŠ¡
        upload_id, uploaded_bytes, next_part = self._get_resume_info(object_key)

        if upload_id:
            logger.info(f"[æ–­ç‚¹å‘ç°] ä»»åŠ¡ID: {upload_id}, "
                        f"å·²ä¸Šä¼ : {uploaded_bytes / 1024 / 1024:.2f} MB, "
                        f"parts: {next_part}")
            logger.info(f"-> ä» offset={uploaded_bytes} å¤„å¼€å§‹ä¸‹è½½")
        else:
            # æ²¡æœ‰æ–­ç‚¹ï¼Œåˆå§‹åŒ–ä¸€ä¸ªæ–°ä»»åŠ¡
            resp = self.client.initiateMultipartUpload(self.bucket, object_key)
            self._check_error(resp)
            upload_id = resp.body.uploadId
            uploaded_bytes = 0
            next_part = 1
            logger.info(f"[æ–°ä»»åŠ¡] å·²åˆ›å»ºä»»åŠ¡ID: {upload_id}")

        # æ‰“åŒ…ä¸Šä¸‹æ–‡è¿”å›
        return UploadContext(object_key, upload_id, uploaded_bytes, next_part)

    def upload_stream(self, context, stream_iterator, total_size=None, mode="ab"):
        """
        ã€ç¬¬äºŒæ­¥ã€‘æ¥æ”¶æ•°æ®æµï¼Œæ‰§è¡Œå¹¶å‘åˆ†ç‰‡ä¸Šä¼ 
        :param context: init_upload è¿”å›çš„ UploadContext å¯¹è±¡
        :param stream_iterator: æ•°æ®æµ (bytes ç”Ÿæˆå™¨)
        :param total_size: (å¯é€‰) å‰©ä½™æ–‡ä»¶å¤§å°ï¼Œç”¨äºè¿›åº¦æ¡æ˜¾ç¤º
        :param mode: "ab" ä»£è¡¨è¿½åŠ (ç»­ä¼ )ï¼Œ"wb" ä»£è¡¨è¦†ç›–(é‡ä¼ )
        :return: æ–‡ä»¶åœ¨ OBS ä¸Šçš„æ€»å¤§å° (int) -> (æ—§offset + æœ¬æ¬¡ä¸Šä¼ é‡)
        """
        if mode == "wb":
            logger.info(f"æ¨¡å¼ä¸º wbï¼Œæ­£åœ¨æ¸…ç†å¹¶é‡ç½®ä»»åŠ¡: {context.key}")
            # 1. é”€æ¯æ—§ä»»åŠ¡
            try:
                self.client.abortMultipartUpload(self.bucket, context.key, context.upload_id)
            except Exception:
                pass
            
            # 2. å¼€å¯æ–°ä»»åŠ¡å¹¶æ›´æ–°ä¸Šä¸‹æ–‡
            resp = self.client.initiateMultipartUpload(self.bucket, context.key)
            self._check_error(resp)
            context.upload_id = resp.body.uploadId
            context.offset = 0
            context.next_part = 1

        logger.info(f"å¼€å§‹æ¥æ”¶æ•°æ®æµï¼Œå†™å…¥: {context.key} (Mode: {mode}, Offset: {context.offset})")

        # åŠ¨æ€è°ƒæ•´åˆ†ç‰‡å¤§å°
        current_part_size = self.part_size
        
        # è®¡ç®—å‰©ä½™å¯ç”¨åˆ†ç‰‡æ•°
        remaining_parts = self.MAX_PARTS - context.next_part + 1
        
        # ç­–ç•¥æ›´æ–°ï¼š
        # 1. å¦‚æœç”¨æˆ·è®¾ç½® total_size -> æ ¹æ®å‰©ä½™æ–‡ä»¶å¤§å°å’Œå‰©ä½™åˆ†ç‰‡æ•°è®¡ç®—
        #    ã€ä¿®å¤ã€‘æ–­ç‚¹ç»­ä¼ æ—¶ä¹Ÿè¦é‡æ–°è®¡ç®—ï¼Œä¸ä»…é™äº offset == 0
        if total_size and total_size > 0:
            # è®¡ç®—å‰©ä½™éœ€è¦ä¸Šä¼ çš„æ•°æ®é‡
            remaining_size = total_size - context.offset if context.offset > 0 else total_size
            
            # ä½¿ç”¨æ›´ä¿å®ˆçš„å®‰å…¨åˆ†ç‰‡æ•° (8000)ï¼Œé¢„ç•™ 20% ç¼“å†²åº”å¯¹ Content-Length ä¸å‡†ç¡®
            # å¯¹äºç»­ä¼ åœºæ™¯ï¼Œä½¿ç”¨å‰©ä½™åˆ†ç‰‡æ•°çš„ 80% ä½œä¸ºå®‰å…¨å€¼
            if context.offset == 0:
                safe_parts_count = self.SAFE_PARTS_COUNT  # æ–°ä»»åŠ¡ç”¨ 8000
            else:
                # ç»­ä¼ æ—¶ï¼Œç”¨å‰©ä½™åˆ†ç‰‡æ•°çš„ 80% ä½œä¸ºå®‰å…¨å€¼
                safe_parts_count = int(remaining_parts * 0.8)
                safe_parts_count = max(safe_parts_count, 100)  # è‡³å°‘ä¿ç•™ 100 ä¸ªåˆ†ç‰‡
            
            # å‘ä¸Šå–æ•´è®¡ç®—æœ€å°åˆ†ç‰‡å¤§å°
            min_part_size = math.ceil(remaining_size / safe_parts_count)
            
            # åªæœ‰å½“è®¡ç®—å‡ºçš„åˆ†ç‰‡å¤§å° > å½“å‰é…ç½®çš„å¤§å°æ—¶ï¼Œæ‰è¿›è¡Œè°ƒæ•´
            if min_part_size > current_part_size:
                if context.offset > 0:
                    logger.warning(f"[æ–­ç‚¹ç»­ä¼ ] å‰©ä½™æ•°æ® ({remaining_size / 1024 / 1024 / 1024:.2f} GB)ï¼Œ"
                                   f"å‰©ä½™åˆ†ç‰‡æ•° {remaining_parts}ï¼Œ"
                                   f"è°ƒæ•´åˆ†ç‰‡å¤§å°: {current_part_size / 1024 / 1024:.0f}MB -> {min_part_size / 1024 / 1024:.0f}MB")
                else:
                    logger.warning(f"æ–‡ä»¶å¤§å° ({total_size / 1024 / 1024 / 1024:.2f} GB) è¾ƒå¤§ï¼Œ"
                                   f"è‡ªåŠ¨è°ƒæ•´åˆ†ç‰‡å¤§å°: {current_part_size / 1024 / 1024:.0f}MB -> {min_part_size / 1024 / 1024:.0f}MB "
                                   f"(ç›®æ ‡åˆ†ç‰‡æ•° ~{safe_parts_count})")
                current_part_size = min_part_size
            
            # ã€æ–°å¢ã€‘ä¸Šä¼ å‰éªŒè¯ï¼šæ£€æŸ¥å‰©ä½™åˆ†ç‰‡æ•°æ˜¯å¦è¶³å¤Ÿ
            estimated_parts_needed = math.ceil(remaining_size / current_part_size)
            if estimated_parts_needed > remaining_parts:
                raise PartLimitExceededError(
                    f"åˆ†ç‰‡æ•°ä¸è¶³ï¼å‰©ä½™åˆ†ç‰‡æ•°: {remaining_parts}ï¼Œ"
                    f"é¢„è®¡éœ€è¦: {estimated_parts_needed}ï¼Œ"
                    f"å‰©ä½™æ•°æ®: {remaining_size / 1024 / 1024 / 1024:.2f} GBï¼Œ"
                    f"å½“å‰åˆ†ç‰‡å¤§å°: {current_part_size / 1024 / 1024:.0f} MBã€‚"
                    f"å»ºè®®ï¼šæ”¾å¼ƒå½“å‰ä»»åŠ¡ (mode='wb') å¹¶ä½¿ç”¨æ›´å¤§çš„åˆ†ç‰‡å¤§å°é‡æ–°ä¸Šä¼ ã€‚",
                    remaining_parts=remaining_parts,
                    estimated_parts=estimated_parts_needed,
                    remaining_size=remaining_size,
                    part_size=current_part_size
                )
                    
        # 2. å¦‚æœç”¨æˆ·æ²¡æœ‰è®¾ç½® total_size (æµå¼) -> é»˜è®¤ 150MB
        else:
            # å¼ºåˆ¶æå‡åˆ° 150MB (æ”¯æŒ ~1.2TB with 8000 parts)ï¼Œæ¨è¿Ÿ"ç´§æ€¥æ‰©å®¹"ä»‹å…¥æ—¶é—´
            DEFAULT_STREAM_PART_SIZE = 150 * 1024 * 1024
            if current_part_size < DEFAULT_STREAM_PART_SIZE:
                logger.info(f"æœªçŸ¥æ€»å¤§å°ï¼Œè‡ªåŠ¨å°†åˆ†ç‰‡å¤§å°ä» {current_part_size / 1024 / 1024:.0f}MB æå‡è‡³ {DEFAULT_STREAM_PART_SIZE / 1024 / 1024:.0f}MB ä»¥æ”¯æŒå¤§æ–‡ä»¶")
                current_part_size = DEFAULT_STREAM_PART_SIZE

        try:
            # æ‰§è¡Œæ ¸å¿ƒä¸Šä¼ é€»è¾‘
            bytes_uploaded = self._process_stream(
                stream_iterator,
                context.key,
                context.upload_id,
                context.next_part,
                total_size,
                current_part_size
            )

            # åªæœ‰æµæ­£å¸¸ç»“æŸï¼Œæ‰æ‰§è¡Œåˆå¹¶æ“ä½œ
            self._complete_upload(context.key, context.upload_id)
            
            # è¿”å› OBS ä¸Šçš„æœ€ç»ˆæ–‡ä»¶å¤§å°
            return context.offset + bytes_uploaded

        except Exception as e:
            logger.error(f"ä¸Šä¼ è¿‡ç¨‹ä¸­æ–­: {e}")
            raise e

    def _get_resume_info(self, key):
        """æŸ¥è¯¢ OBS æœåŠ¡ç«¯æ˜¯å¦å­˜åœ¨æœªå®Œæˆçš„åˆ†æ®µä»»åŠ¡"""
        # 1. åˆ—å‡ºæ¡¶å†…æ‰€æœ‰åˆ†æ®µä»»åŠ¡
        list_req = ListMultipartUploadsRequest(prefix=key)
        resp = self.client.listMultipartUploads(self.bucket, multipart=list_req)

        target_id = None
        # æ‰¾åˆ° key å®Œå…¨åŒ¹é…çš„ä»»åŠ¡ (å–æœ€æ–°çš„ä¸€ä¸ª)
        if resp.status < 300 and resp.body.upload:
            for upload in resp.body.upload:
                if upload.key == key:
                    target_id = upload.uploadId
                    # æ³¨æ„ï¼šè¿™é‡Œä¸ breakï¼Œç»§ç»­æ‰¾å¯èƒ½æ˜¯ä¸ºäº†æ‰¾æœ€æ–°çš„ï¼Œæˆ–è€…é»˜è®¤å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„
                    # åœ¨æœ¬ç®€ç‰ˆå®ç°ä¸­ï¼Œå–åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹é€šå¸¸å³å¯
                    break

        if not target_id:
            return None, 0, 1

        # 2. ç»Ÿè®¡è¯¥ä»»åŠ¡å·²ä¸Šä¼ çš„åˆ†ç‰‡ï¼Œè®¡ç®— offset
        uploaded_bytes = 0
        next_part = 1
        marker = None

        # åˆ†é¡µæ‹‰å–æ‰€æœ‰å·²ä¸Šä¼ åˆ†ç‰‡
        while True:
            parts_resp = self.client.listParts(self.bucket, key, target_id, partNumberMarker=marker)
            if parts_resp.status >= 300:
                logger.warning(f"æŸ¥è¯¢åˆ†ç‰‡å¤±è´¥: {parts_resp.errorMessage}")
                return None, 0, 1  # é™çº§ä¸ºæ–°ä»»åŠ¡

            for part in parts_resp.body.parts:
                # ç®€å•æ ¡éªŒï¼šå‡è®¾åˆ†ç‰‡æ˜¯è¿ç»­ä¸Šä¼ çš„ï¼Œä¸”å¤§å°ç¬¦åˆå½“å‰é…ç½®
                # å¦‚æœå†å²åˆ†ç‰‡å¤§å°å’Œå½“å‰é…ç½®ä¸ä¸€è‡´ (é™¤äº†æœ€åä¸€ä¸ª)ï¼Œå¯èƒ½å¯¼è‡´ç»­ä¼ é”™ä½
                if part.partNumber == next_part:
                    # ä¸¥æ ¼æ¨¡å¼ä¸‹åº”æ ¡éªŒ part.size == self.part_size
                    uploaded_bytes += part.size
                    next_part += 1

            if not parts_resp.body.isTruncated:
                break
            marker = parts_resp.body.nextPartNumberMarker

        return target_id, uploaded_bytes, next_part

    def _process_stream(self, iterator, key, uid, start_part, total_size, part_size):
        """è¯»å–æµ -> ç¼“å†² -> æäº¤çº¿ç¨‹æ±  (æ”¯æŒåŠ¨æ€åˆ†ç‰‡å¤§å°è°ƒæ•´)"""
        buffer = BytesIO()
        part_number = start_part
        total_stream_bytes = 0
        current_part_size = part_size  # ä½¿ç”¨å±€éƒ¨å˜é‡æ”¯æŒåŠ¨æ€è°ƒæ•´

        # å¦‚æœæ˜¯ç»­ä¼ ï¼Œå…ˆæ‹‰å–å†å²åˆ†ç‰‡ä¿¡æ¯
        parts_map = {}
        if start_part > 1:
            parts_map = self._fetch_uploaded_parts_map(key, uid)

        # è¿›åº¦æ¡è®¾ç½®
        pbar = None
        if tqdm:
            current_uploaded = (start_part - 1) * part_size
            pbar = tqdm(
                total=total_size,
                initial=current_uploaded,
                unit='B',
                unit_scale=True,
                desc=f"ğŸš€ Uploading {os.path.basename(key)}",
                mininterval=1,
                position=0,
                dynamic_ncols=True
            )

        # ä½¿ç”¨é—­åŒ…å˜é‡æ¥è®°å½•ä¸Šä¸€æ¬¡æŠ¥è­¦çš„åˆ†ç‰‡å·ï¼Œé¿å…é‡å¤åˆ·å±
        last_warned_part = [-1]

        def calculate_dynamic_part_size(current_part, current_size, uploaded_bytes, total_file_size):
            """è®¡ç®—åŠ¨æ€åˆ†ç‰‡å¤§å°ï¼ˆåœ¨æ¯ä¸ªåˆ†ç‰‡ä¸Šä¼ å‰è°ƒç”¨ï¼‰
            åªæœ‰åœ¨åˆ†ç‰‡å·æ¥è¿‘ SAFETY_THRESHOLD æ—¶æ‰è°ƒæ•´
            """
            # æ£€æŸ¥æ˜¯å¦æ¥è¿‘å®‰å…¨é˜ˆå€¼
            if current_part >= self.SAFETY_THRESHOLD:
                # æƒ…å†µA: å·²çŸ¥æ€»å¤§å° -> ç²¾ç¡®è®¡ç®—å‰©ä½™éœ€è¦çš„å¹³å‡å¤§å°
                if total_file_size:
                    remaining_parts = self.MAX_PARTS - current_part
                    estimated_remaining = total_file_size - uploaded_bytes

                    if remaining_parts > 0 and estimated_remaining > 0:
                        new_size = estimated_remaining // remaining_parts
                        if new_size > current_size:
                            adjusted_size = max(new_size, self.MIN_PART_SIZE)
                            
                            if last_warned_part[0] != current_part:
                                logger.warning(f"åˆ†ç‰‡ {current_part} æ¥è¿‘é˜ˆå€¼ {self.SAFETY_THRESHOLD} (From TotalSize)ï¼ŒåŠ¨æ€è°ƒæ•´åˆ†ç‰‡å¤§å°ä¸º {adjusted_size} bytes")
                                last_warned_part[0] = current_part
                                
                            return adjusted_size
                
                # æƒ…å†µB: æœªçŸ¥æ€»å¤§å° (æµå¼) -> ç´§æ€¥æ‰©å®¹
                else:
                    # ç­–ç•¥ä¼˜åŒ–ï¼šä» 5000 åˆ†ç‰‡å¼€å§‹ææ—©ä»‹å…¥
                    ratio = (current_part - self.SAFETY_THRESHOLD) / (self.MAX_PARTS - self.SAFETY_THRESHOLD)
                    
                    multiplier = 1
                    if ratio > 0.8: # > 9000
                        multiplier = 10 
                    elif ratio > 0.6: # > 8000
                        multiplier = 5 
                    elif ratio > 0.4: # > 7000
                        multiplier = 2
                    elif ratio >= 0: # 5000 ~ 7000
                        multiplier = 1.5 
                        
                    new_size = int(current_size * multiplier)
                    if new_size > current_size:
                        if last_warned_part[0] != current_part:
                            logger.warning(f"åˆ†ç‰‡ {current_part} æ¥è¿‘é˜ˆå€¼ {self.SAFETY_THRESHOLD} (Unknown TotalSize)ï¼Œå¹³æ»‘æ‰©å®¹: {current_size / 1024 / 1024:.0f}MB -> {new_size / 1024 / 1024:.0f}MB")
                            last_warned_part[0] = current_part
                        return new_size

            return current_size

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {}  # {future: part_number}

            try:
                for chunk in iterator:
                    if chunk:
                        total_stream_bytes += len(chunk)
                        buffer.write(chunk)

                        if pbar is not None:
                            pbar.update(len(chunk))

                        # åœ¨æäº¤åˆ†ç‰‡å‰ï¼ŒåŠ¨æ€è®¡ç®—åˆ†ç‰‡å¤§å°
                        effective_part_size = calculate_dynamic_part_size(
                            part_number, current_part_size, total_stream_bytes, total_size
                        )

                        # ç¼“å†²åŒºè¾¾åˆ°åˆ†ç‰‡å¤§å° -> æäº¤ä¸Šä¼ 
                        if buffer.tell() >= effective_part_size:
                            # å°†æŒ‡é’ˆé‡ç½®åˆ°å¼€å¤´ï¼Œå‡†å¤‡è¯»å–
                            buffer.seek(0)

                            while True:
                                # è®¡ç®—å½“å‰å‰©ä½™å¯è¯»å­—èŠ‚æ•°
                                remaining_len = buffer.getbuffer().nbytes - buffer.tell()

                                # å¦‚æœå‰©ä½™æ•°æ®ä¸è¶³ä¸€ä¸ªåˆ†ç‰‡ï¼Œåœæ­¢å¾ªç¯
                                if remaining_len < effective_part_size:
                                    break

                                # è¯»å–ä¸€ä¸ªå®Œæ•´åˆ†ç‰‡çš„æ•°æ®
                                data = buffer.read(effective_part_size)

                                # æäº¤ä»»åŠ¡
                                f = executor.submit(self._upload_part_with_retry, key, uid, part_number, data)
                                futures[f] = part_number

                                part_number += 1

                                # æµæ§ï¼šé˜²æ­¢å†…å­˜æº¢å‡º
                                if len(futures) >= self.max_workers * 2:
                                    self._wait_and_collect(futures, parts_map)

                                # ä¸ºä¸‹ä¸€ä¸ªåˆ†ç‰‡é‡æ–°è®¡ç®—å¤§å°
                                effective_part_size = calculate_dynamic_part_size(
                                    part_number, current_part_size, total_stream_bytes, total_size
                                )

                            # è¯»å–å‰©ä½™çš„æ‰€æœ‰æ•°æ®
                            remaining_data = buffer.read()
                            # é‡ç½®ç¼“å†²ï¼Œå¹¶å°†å‰©ä½™æ•°æ®å†™å…¥
                            buffer = BytesIO()
                            buffer.write(remaining_data)

                # å¤„ç†å‰©ä½™æ•°æ®ï¼ˆæœ€åä¸€ä¸ªåˆ†ç‰‡å¯ä»¥æ˜¯ä»»æ„å¤§å°ï¼‰
                if buffer.tell() > 0:
                    f = executor.submit(self._upload_part_with_retry, key, uid, part_number, buffer.getvalue())
                    futures[f] = part_number

                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for f in as_completed(futures):
                    p_num = futures[f]
                    etag = f.result()
                    parts_map[p_num] = etag

            except Exception as e:
                raise e
            finally:
                if pbar is not None:
                    pbar.close()

        # ä¿å­˜åˆ†ç‰‡æ˜ å°„ä¾›åˆå¹¶ä½¿ç”¨
        self._final_parts_map = parts_map
        return total_stream_bytes

    def _upload_part_with_retry(self, key, uid, p_num, data):
        """å¸¦é‡è¯•æœºåˆ¶çš„å•ä¸ªåˆ†ç‰‡ä¸Šä¼ ï¼ˆå¢åŠ è¯¦ç»†æ—¥å¿—ï¼‰"""
        # åˆ†ç‰‡å·æ ¡éªŒï¼šç¡®ä¿ä¸è¶…è¿‡ OBS é™åˆ¶
        if p_num > self.MAX_PARTS:
            raise PartLimitExceededError(
                f"åˆ†ç‰‡å· {p_num} è¶…è¿‡ä¸Šé™ {self.MAX_PARTS}ï¼Œæ— æ³•ç»§ç»­ä¸Šä¼ ",
                remaining_parts=0,
                estimated_parts=p_num
            )

        data_len = len(data)

        for i in range(self.max_retries):
            try:
                start_time = time.time()
                # æ‰§è¡Œä¸Šä¼ 
                resp = self.client.uploadPart(
                    bucketName=self.bucket, objectKey=key, partNumber=p_num,
                    uploadId=uid, content=data, partSize=data_len
                )
                if resp.status < 300:
                    # è®¡ç®—è€—æ—¶å’Œé€Ÿåº¦
                    duration = time.time() - start_time
                    speed = (data_len / 1024 / 1024) / duration if duration > 0 else 0
                    # âœ… æ‰“å°è¯¦ç»†çš„æˆåŠŸæ—¥å¿—
                    logger.info(f"åˆ†ç‰‡ #{p_num} ä¸Šä¼ æˆåŠŸ | "
                                f"å¤§å°: {data_len / 1024 / 1024:.2f}MB | "
                                f"è€—æ—¶: {duration:.1f}s | "
                                f"é€Ÿåº¦: {speed:.1f}MB/s")
                    return resp.body.etag
                else:
                    logger.warning(
                        f"âš ï¸ åˆ†ç‰‡ #{p_num} ä¸Šä¼ å¤±è´¥ (HTTP {resp.status}, Code: {resp.errorCode}, Msg: {resp.errorMessage})ï¼Œæ­£åœ¨é‡è¯• {i + 1}/{self.max_retries}...")

            except Exception as ex:
                logger.warning(f"âŒ åˆ†ç‰‡ #{p_num} å‘ç”Ÿå¼‚å¸¸: {ex}ï¼Œæ­£åœ¨é‡è¯• {i + 1}/{self.max_retries}...")
            # å¤±è´¥åç¨å¾®ç­‰å¾…ä¸€ä¸‹å†é‡è¯•
            time.sleep(1 * (i + 1))
        raise PartUploadError(f"åˆ†ç‰‡ #{p_num} åœ¨ {self.max_retries} æ¬¡å°è¯•åæœ€ç»ˆå¤±è´¥", part_number=p_num)

    def _wait_and_collect(self, futures, parts_map):
        """ç­‰å¾…éƒ¨åˆ†ä»»åŠ¡å®Œæˆï¼Œå›æ”¶å†…å­˜ï¼Œæ”¶é›† ETag"""
        done, _ = wait(futures.keys(), return_when=FIRST_COMPLETED)
        for f in done:
            p_num = futures.pop(f)
            try:
                parts_map[p_num] = f.result()
            except Exception as e:
                # è¿™é‡Œæ•è·å¼‚å¸¸æ˜¯ä¸ºäº†ä¸æ‰“æ–­ä¸»å¾ªç¯ï¼Œ
                # ä½†å®é™…ä¸Šå¦‚æœåˆ†ç‰‡å¤±è´¥ï¼Œæœ€ç»ˆåˆå¹¶ä¼šå¤±è´¥ï¼Œæˆ–è€…ä¸Šé¢å·²ç»æŠ›å‡ºäº†
                raise e

    def _fetch_uploaded_parts_map(self, key, uid):
        """è·å–æœåŠ¡ç«¯å·²æœ‰çš„åˆ†ç‰‡ä¿¡æ¯ (PartNum -> ETag)"""
        mapping = {}
        marker = None
        while True:
            resp = self.client.listParts(self.bucket, key, uid, partNumberMarker=marker)
            if resp.status >= 300: break
            for p in resp.body.parts:
                mapping[p.partNumber] = p.etag
            if not resp.body.isTruncated: break
            marker = resp.body.nextPartNumberMarker
        return mapping

    def _complete_upload(self, key, uid):
        """åˆå¹¶åˆ†ç‰‡"""
        logger.info("æµä¼ è¾“ç»“æŸï¼Œæ­£åœ¨è¯·æ±‚åˆå¹¶åˆ†ç‰‡...")

        # ä½¿ç”¨æœ€æ–°çš„ parts_map (åŒ…å«å†å²çš„å’Œæœ¬æ¬¡ä¸Šä¼ çš„)
        # å¦‚æœ _process_stream æˆåŠŸæ‰§è¡Œï¼Œself._final_parts_map åº”è¯¥æœ‰å®Œæ•´æ•°æ®
        # ä¸ºäº†ä¿é™©ï¼Œè¿™é‡Œå¯ä»¥ä½¿ç”¨ _fetch_uploaded_parts_map å†æ¬¡ä»æœåŠ¡ç«¯ç¡®è®¤ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„ map
        # è¿™é‡Œç›´æ¥ä½¿ç”¨å†…å­˜ç´¯ç§¯çš„ mapï¼Œå®ƒæ˜¯æœ€å‡†ç¡®çš„ï¼ˆåŒ…å«æœ¬æ¬¡ä¸Šä¼ ç»“æœï¼‰
        if not hasattr(self, '_final_parts_map') or not self._final_parts_map:
            # å…œåº•ï¼šå¦‚æœå†…å­˜æ²¡æ•°æ®ï¼ˆæ¯”å¦‚æµæ˜¯ç©ºçš„ï¼‰ï¼Œå°è¯•æŸ¥æœåŠ¡ç«¯
            self._final_parts_map = self._fetch_uploaded_parts_map(key, uid)

        if not self._final_parts_map:
            # ç©ºæ•°æ®æµï¼šå–æ¶ˆåˆ†ç‰‡ä¸Šä¼ ä»»åŠ¡ï¼Œé™é»˜è¿”å›
            logger.warning(f"âš ï¸ æ•°æ®æµä¸ºç©ºï¼ˆ0 å­—èŠ‚ï¼‰ï¼Œè·³è¿‡ä¸Šä¼ : {key}")
            try:
                self.client.abortMultipartUpload(self.bucket, key, uid)
                logger.info(f"å·²å–æ¶ˆç©ºä»»åŠ¡: {key}")
            except Exception:
                pass  # å¿½ç•¥å–æ¶ˆå¤±è´¥
            return

        # æ„é€ åˆå¹¶è¯·æ±‚åˆ—è¡¨ (å¿…é¡»æŒ‰ PartNum æ’åº)
        sorted_parts = [
            CompletePart(partNum=k, etag=v)
            for k, v in sorted(self._final_parts_map.items())
        ]

        resp = self.client.completeMultipartUpload(
            self.bucket, key, uid,
            CompleteMultipartUploadRequest(sorted_parts)
        )
        self._check_error(resp)
        logger.info(f"âœ… ä¸Šä¼ æˆåŠŸ: {key}")

    def _check_error(self, resp):
        if resp.status >= 300:
            raise Exception(f"OBS Error {resp.errorCode}: {resp.errorMessage}")