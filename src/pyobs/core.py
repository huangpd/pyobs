import os
import time
import logging
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED
from obs import ObsClient, CompleteMultipartUploadRequest, CompletePart, ListMultipartUploadsRequest
from tqdm import tqdm
import http.client
# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("ObsStream")

# ==========================================
# ğŸ©¹ çŒ´å­è¡¥ä¸ (Monkey Patch)
# ä¿®å¤ huaweicloud-sdk-python-obs SSL è¿æ¥å‚æ•°æŠ¥é”™é—®é¢˜
# ==========================================
def _patched_get_server_connection(self, is_secure, server, port, proxy_host, proxy_port):
    """è¦†ç›– SDK åŸæœ‰çš„è¿æ¥åˆ›å»ºæ–¹æ³•ï¼Œç§»é™¤ä¸æ”¯æŒçš„ check_hostname å‚æ•°"""
    if proxy_host is not None and proxy_port is not None:
        server = proxy_host
        port = proxy_port

    if is_secure:
        # å…³é”®ä¿®æ”¹ï¼šç›´æ¥ç§»é™¤ check_hostname å‚æ•°
        # Python 3 çš„ HTTPSConnection ä¼šè‡ªåŠ¨ä½¿ç”¨ context ä¸­çš„é…ç½®
        try:
            conn = http.client.HTTPSConnection(
                server,
                port=port,
                timeout=self.timeout,
                context=self.context
            )
        except TypeError:
            # å…œåº•ï¼šä¸‡ä¸€ context ä¹Ÿä¸æ”¯æŒï¼ˆæå°‘è§ï¼‰ï¼Œåˆ™ä¸ä¼  context
            conn = http.client.HTTPSConnection(
                server,
                port=port,
                timeout=self.timeout
            )
    else:
        conn = http.client.HTTPConnection(server, port=port, timeout=self.timeout)

    return conn

# åº”ç”¨è¡¥ä¸ï¼šæ›¿æ¢ ObsClient ç±»çš„å†…éƒ¨æ–¹æ³•
ObsClient._get_server_connection_use_http1x = _patched_get_server_connection
# ==========================================
# ğŸ©¹ çŒ´å­è¡¥ä¸ ç»“æŸ
# ==========================================

class UploadContext:
    """
    ä¸Šä¼ ä¸Šä¸‹æ–‡ï¼šç”¨äºåœ¨åˆå§‹åŒ–å’Œå®é™…ä¸Šä¼ ä¹‹é—´ä¼ é€’çŠ¶æ€
    """

    def __init__(self, object_key, upload_id, offset, next_part):
        self.key = object_key
        self.upload_id = upload_id  # OBS å†…éƒ¨ä»»åŠ¡ ID (ç”¨äºç»­ä¼ )
        self.offset = offset  # å·²ä¸Šä¼ å­—èŠ‚æ•° (ç”¨äºå‘Šè¯‰ä¸‹è½½å™¨ Range)
        self.next_part = next_part  # ä¸‹ä¸€ä¸ªåˆ†ç‰‡å· (ç”¨äºåˆ†ç‰‡æ’åº)


class StreamUploader:
    """
    åä¸ºäº‘ OBS æµå¼ä¸Šä¼ å·¥å…· (æ”¯æŒæ–­ç‚¹ç»­ä¼ )
    """

    # --- åˆ†ç‰‡ä¸Šä¼ å¸¸é‡ ---
    MAX_PARTS = 10000  # OBS åˆ†ç‰‡ä¸Šä¼ çš„ç¡¬æ€§ä¸Šé™
    SAFETY_THRESHOLD = 8000  # å®‰å…¨é˜ˆå€¼ï¼Œè¾¾åˆ°æ­¤åˆ†ç‰‡æ•°æ—¶å¼€å§‹åŠ¨æ€è°ƒæ•´
    MIN_PART_SIZE = 5 * 1024 * 1024  # 5MB (S3/OBS åè®®è¦æ±‚å‰ N-1 ä¸ªåˆ†ç‰‡ â‰¥ 5MB)

    def __init__(self, ak=None, sk=None, server=None, bucket_name=None,
                 part_size=20 * 1024 * 1024):
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼Œæ”¯æŒæ— å‚åˆå§‹åŒ–
        self.ak = ak or os.getenv("OBS_AK")
        self.sk = sk or os.getenv("OBS_SK")
        self.server = server or os.getenv("OBS_SERVER")
        self.bucket = bucket_name or os.getenv("OBS_BUCKET")

        if not all([self.ak, self.sk, self.server, self.bucket]):
            raise ValueError("å¿…é¡»æä¾› AK, SK, Server å’Œ Bucket Name (å¯é€šè¿‡å‚æ•°æˆ–ç¯å¢ƒå˜é‡)")

        self.client = ObsClient(access_key_id=self.ak, secret_access_key=self.sk, server=self.server)

        # --- æ ¸å¿ƒé…ç½® ---
        # åˆ†ç‰‡å¤§å°é»˜è®¤ 20MBï¼Œæ—¢èƒ½ä¿è¯å¹¶å‘åº¦ï¼Œåˆé€‚é…æµå¼åœºæ™¯
        self.part_size = part_size
        self.max_workers = 5  # å¹¶å‘ä¸Šä¼ çº¿ç¨‹æ•°
        self.max_retries = 5  # å•ä¸ªåˆ†ç‰‡ä¸Šä¼ å¤±è´¥é‡è¯•æ¬¡æ•°

    def init_upload(self, object_key):
        """
        ã€ç¬¬ä¸€æ­¥ã€‘åˆå§‹åŒ–ä¸Šä¼ ä»»åŠ¡ï¼Œæ¢æµ‹æ–­ç‚¹
        :param object_key: OBS ç›®æ ‡è·¯å¾„
        :return: UploadContext å¯¹è±¡
        """
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–ä»»åŠ¡: {object_key} ...")

        # 1. å» OBS æŸ¥æœ‰æ²¡æœ‰æœªå®Œæˆçš„ä»»åŠ¡
        upload_id, uploaded_bytes, next_part = self._get_resume_info(object_key)

        if upload_id:
            logger.info(f"[æ–­ç‚¹å‘ç°] ä»»åŠ¡ID: {upload_id}, "
                        f"å·²ä¸Šä¼ : {uploaded_bytes / 1024 / 1024:.2f} MB, "
                        f"parts: {next_part}")
            logger.info(f"-> ä» offset={uploaded_bytes} å¤„å¼€å§‹ä¸‹è½½")
        else:
            # æ²¡æœ‰æ–­ç‚¹ï¼Œåˆå§‹åŒ–ä¸€ä¸ªæ–°ä»»åŠ¡
            resp = self.client.initiateMultipartUpload(self.bucket, object_key)
            self._check_error(resp)
            upload_id = resp.body.uploadId
            uploaded_bytes = 0
            next_part = 1
            logger.info(f"[æ–°ä»»åŠ¡] å·²åˆ›å»ºä»»åŠ¡ID: {upload_id}")

        # æ‰“åŒ…ä¸Šä¸‹æ–‡è¿”å›
        return UploadContext(object_key, upload_id, uploaded_bytes, next_part)

    def upload_stream(self, context, stream_iterator, total_size=None, mode="ab"):
        """
        ã€ç¬¬äºŒæ­¥ã€‘æ¥æ”¶æ•°æ®æµï¼Œæ‰§è¡Œå¹¶å‘åˆ†ç‰‡ä¸Šä¼ 
        :param context: init_upload è¿”å›çš„ UploadContext å¯¹è±¡
        :param stream_iterator: æ•°æ®æµ (bytes ç”Ÿæˆå™¨)
        :param total_size: (å¯é€‰) å‰©ä½™æ–‡ä»¶å¤§å°ï¼Œç”¨äºè¿›åº¦æ¡æ˜¾ç¤º
        :param mode: "ab" ä»£è¡¨è¿½åŠ (ç»­ä¼ )ï¼Œ"wb" ä»£è¡¨è¦†ç›–(é‡ä¼ )
        :return: æ–‡ä»¶åœ¨ OBS ä¸Šçš„æ€»å¤§å° (int) -> (æ—§offset + æœ¬æ¬¡ä¸Šä¼ é‡)
        """
        if mode == "wb":
            logger.info(f"æ¨¡å¼ä¸º wbï¼Œæ­£åœ¨æ¸…ç†å¹¶é‡ç½®ä»»åŠ¡: {context.key}")
            # 1. é”€æ¯æ—§ä»»åŠ¡
            try:
                self.client.abortMultipartUpload(self.bucket, context.key, context.upload_id)
            except Exception:
                pass
            
            # 2. å¼€å¯æ–°ä»»åŠ¡å¹¶æ›´æ–°ä¸Šä¸‹æ–‡
            resp = self.client.initiateMultipartUpload(self.bucket, context.key)
            self._check_error(resp)
            context.upload_id = resp.body.uploadId
            context.offset = 0
            context.next_part = 1

        logger.info(f"å¼€å§‹æ¥æ”¶æ•°æ®æµï¼Œå†™å…¥: {context.key} (Mode: {mode}, Offset: {context.offset})")

        # åŠ¨æ€è°ƒæ•´åˆ†ç‰‡å¤§å°
        current_part_size = self.part_size
        # å¦‚æœç”¨æˆ·ä¼ äº† total_sizeï¼Œé¢„å…ˆè®¡ç®—åˆé€‚çš„åˆ†ç‰‡å¤§å°
        if total_size and total_size > 0 and context.offset == 0:
            # OBS åˆ†ç‰‡æ•°é‡ä¸Šé™ä¸º MAX_PARTSã€‚è¿™é‡Œé™¤ä»¥ SAFETY_THRESHOLD é¢„ç•™å®‰å…¨ Bufferï¼Œé˜²æ­¢è¾¹ç¼˜æº¢å‡º
            min_part_size = (total_size // self.SAFETY_THRESHOLD) + 1
            if min_part_size > current_part_size:
                logger.warning(f"æ–‡ä»¶å¤§å° ({total_size}) è¶…è¿‡å½“å‰åˆ†ç‰‡é™åˆ¶ï¼Œè‡ªåŠ¨è°ƒæ•´åˆ†ç‰‡å¤§å°ä¸º {min_part_size} bytes")
                current_part_size = min_part_size

        try:
            # æ‰§è¡Œæ ¸å¿ƒä¸Šä¼ é€»è¾‘
            bytes_uploaded = self._process_stream(
                stream_iterator,
                context.key,
                context.upload_id,
                context.next_part,
                total_size,
                current_part_size
            )

            # åªæœ‰æµæ­£å¸¸ç»“æŸï¼Œæ‰æ‰§è¡Œåˆå¹¶æ“ä½œ
            self._complete_upload(context.key, context.upload_id)
            
            # è¿”å› OBS ä¸Šçš„æœ€ç»ˆæ–‡ä»¶å¤§å°
            return context.offset + bytes_uploaded

        except Exception as e:
            logger.error(f"ä¸Šä¼ è¿‡ç¨‹ä¸­æ–­: {e}")
            raise e

    def _get_resume_info(self, key):
        """æŸ¥è¯¢ OBS æœåŠ¡ç«¯æ˜¯å¦å­˜åœ¨æœªå®Œæˆçš„åˆ†æ®µä»»åŠ¡"""
        # 1. åˆ—å‡ºæ¡¶å†…æ‰€æœ‰åˆ†æ®µä»»åŠ¡
        list_req = ListMultipartUploadsRequest(prefix=key)
        resp = self.client.listMultipartUploads(self.bucket, multipart=list_req)

        target_id = None
        # æ‰¾åˆ° key å®Œå…¨åŒ¹é…çš„ä»»åŠ¡ (å–æœ€æ–°çš„ä¸€ä¸ª)
        if resp.status < 300 and resp.body.upload:
            for upload in resp.body.upload:
                if upload.key == key:
                    target_id = upload.uploadId
                    # æ³¨æ„ï¼šè¿™é‡Œä¸ breakï¼Œç»§ç»­æ‰¾å¯èƒ½æ˜¯ä¸ºäº†æ‰¾æœ€æ–°çš„ï¼Œæˆ–è€…é»˜è®¤å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„
                    # åœ¨æœ¬ç®€ç‰ˆå®ç°ä¸­ï¼Œå–åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹é€šå¸¸å³å¯
                    break

        if not target_id:
            return None, 0, 1

        # 2. ç»Ÿè®¡è¯¥ä»»åŠ¡å·²ä¸Šä¼ çš„åˆ†ç‰‡ï¼Œè®¡ç®— offset
        uploaded_bytes = 0
        next_part = 1
        marker = None

        # åˆ†é¡µæ‹‰å–æ‰€æœ‰å·²ä¸Šä¼ åˆ†ç‰‡
        while True:
            parts_resp = self.client.listParts(self.bucket, key, target_id, partNumberMarker=marker)
            if parts_resp.status >= 300:
                logger.warning(f"æŸ¥è¯¢åˆ†ç‰‡å¤±è´¥: {parts_resp.errorMessage}")
                return None, 0, 1  # é™çº§ä¸ºæ–°ä»»åŠ¡

            for part in parts_resp.body.parts:
                # ç®€å•æ ¡éªŒï¼šå‡è®¾åˆ†ç‰‡æ˜¯è¿ç»­ä¸Šä¼ çš„ï¼Œä¸”å¤§å°ç¬¦åˆå½“å‰é…ç½®
                # å¦‚æœå†å²åˆ†ç‰‡å¤§å°å’Œå½“å‰é…ç½®ä¸ä¸€è‡´ (é™¤äº†æœ€åä¸€ä¸ª)ï¼Œå¯èƒ½å¯¼è‡´ç»­ä¼ é”™ä½
                if part.partNumber == next_part:
                    # ä¸¥æ ¼æ¨¡å¼ä¸‹åº”æ ¡éªŒ part.size == self.part_size
                    uploaded_bytes += part.size
                    next_part += 1

            if not parts_resp.body.isTruncated:
                break
            marker = parts_resp.body.nextPartNumberMarker

        return target_id, uploaded_bytes, next_part

    def _process_stream(self, iterator, key, uid, start_part, total_size, part_size):
        """è¯»å–æµ -> ç¼“å†² -> æäº¤çº¿ç¨‹æ±  (æ”¯æŒåŠ¨æ€åˆ†ç‰‡å¤§å°è°ƒæ•´)"""
        buffer = BytesIO()
        part_number = start_part
        total_stream_bytes = 0
        current_part_size = part_size  # ä½¿ç”¨å±€éƒ¨å˜é‡æ”¯æŒåŠ¨æ€è°ƒæ•´

        # å¦‚æœæ˜¯ç»­ä¼ ï¼Œå…ˆæ‹‰å–å†å²åˆ†ç‰‡ä¿¡æ¯
        parts_map = {}
        if start_part > 1:
            parts_map = self._fetch_uploaded_parts_map(key, uid)

        # è¿›åº¦æ¡è®¾ç½®
        pbar = None
        if tqdm:
            current_uploaded = (start_part - 1) * part_size
            pbar = tqdm(
                total=total_size,
                initial=current_uploaded,
                unit='B',
                unit_scale=True,
                desc=f"ğŸš€ Uploading {os.path.basename(key)}",
                mininterval=5,
                position=0,
                dynamic_ncols=True
            )

        def calculate_dynamic_part_size(current_part, current_size, uploaded_bytes, total_file_size):
            """è®¡ç®—åŠ¨æ€åˆ†ç‰‡å¤§å°ï¼ˆåœ¨æ¯ä¸ªåˆ†ç‰‡ä¸Šä¼ å‰è°ƒç”¨ï¼‰
            åªæœ‰åœ¨åˆ†ç‰‡å·æ¥è¿‘ SAFETY_THRESHOLD æ—¶æ‰è°ƒæ•´
            """
            # å¦‚æœæ²¡æœ‰ total_sizeï¼Œä¸è°ƒæ•´
            if not total_file_size:
                return current_size

            # æ£€æŸ¥æ˜¯å¦æ¥è¿‘å®‰å…¨é˜ˆå€¼
            if current_part >= self.SAFETY_THRESHOLD:
                remaining_parts = self.MAX_PARTS - current_part
                estimated_remaining = total_file_size - uploaded_bytes

                if remaining_parts > 0 and estimated_remaining > 0:
                    new_size = estimated_remaining // remaining_parts
                    if new_size > current_size:
                        adjusted_size = max(new_size, self.MIN_PART_SIZE)
                        logger.warning(f"åˆ†ç‰‡ {current_part} æ¥è¿‘é˜ˆå€¼ {self.SAFETY_THRESHOLD}ï¼ŒåŠ¨æ€è°ƒæ•´åˆ†ç‰‡å¤§å°ä¸º {adjusted_size} bytes")
                        return adjusted_size
            return current_size

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {}  # {future: part_number}

            try:
                for chunk in iterator:
                    if chunk:
                        total_stream_bytes += len(chunk)
                        buffer.write(chunk)

                        if pbar is not None:
                            pbar.update(len(chunk))

                        # åœ¨æäº¤åˆ†ç‰‡å‰ï¼ŒåŠ¨æ€è®¡ç®—åˆ†ç‰‡å¤§å°
                        effective_part_size = calculate_dynamic_part_size(
                            part_number, current_part_size, total_stream_bytes, total_size
                        )

                        # ç¼“å†²åŒºè¾¾åˆ°åˆ†ç‰‡å¤§å° -> æäº¤ä¸Šä¼ 
                        if buffer.tell() >= effective_part_size:
                            data = buffer.getvalue()

                            f = executor.submit(self._upload_part_with_retry, key, uid, part_number, data)
                            futures[f] = part_number

                            part_number += 1
                            buffer = BytesIO()  # é‡ç½®ç¼“å†²

                            # æµæ§ï¼šé˜²æ­¢å†…å­˜æº¢å‡º
                            if len(futures) >= self.max_workers * 2:
                                self._wait_and_collect(futures, parts_map)

                # å¤„ç†å‰©ä½™æ•°æ®ï¼ˆæœ€åä¸€ä¸ªåˆ†ç‰‡å¯ä»¥æ˜¯ä»»æ„å¤§å°ï¼‰
                if buffer.tell() > 0:
                    f = executor.submit(self._upload_part_with_retry, key, uid, part_number, buffer.getvalue())
                    futures[f] = part_number

                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                for f in as_completed(futures):
                    p_num = futures[f]
                    etag = f.result()
                    parts_map[p_num] = etag

            except Exception as e:
                raise e
            finally:
                if pbar is not None:
                    pbar.close()

        # ä¿å­˜åˆ†ç‰‡æ˜ å°„ä¾›åˆå¹¶ä½¿ç”¨
        self._final_parts_map = parts_map
        return total_stream_bytes

    def _upload_part_with_retry(self, key, uid, p_num, data):
        """å¸¦é‡è¯•æœºåˆ¶çš„å•ä¸ªåˆ†ç‰‡ä¸Šä¼ ï¼ˆå¢åŠ è¯¦ç»†æ—¥å¿—ï¼‰"""
        # åˆ†ç‰‡å·æ ¡éªŒï¼šç¡®ä¿ä¸è¶…è¿‡ OBS é™åˆ¶
        if p_num > self.MAX_PARTS:
            raise Exception(f"åˆ†ç‰‡å· {p_num} è¶…è¿‡ä¸Šé™ {self.MAX_PARTS}ï¼Œæ— æ³•ç»§ç»­ä¸Šä¼ ")

        data_len = len(data)

        for i in range(self.max_retries):
            try:
                start_time = time.time()
                # æ‰§è¡Œä¸Šä¼ 
                resp = self.client.uploadPart(
                    bucketName=self.bucket, objectKey=key, partNumber=p_num,
                    uploadId=uid, content=data, partSize=data_len
                )
                if resp.status < 300:
                    # è®¡ç®—è€—æ—¶å’Œé€Ÿåº¦
                    duration = time.time() - start_time
                    speed = (data_len / 1024 / 1024) / duration if duration > 0 else 0
                    # âœ… æ‰“å°è¯¦ç»†çš„æˆåŠŸæ—¥å¿—
                    logger.debug(f"åˆ†ç‰‡ #{p_num} ä¸Šä¼ æˆåŠŸ | "
                                f"å¤§å°: {data_len / 1024 / 1024:.2f}MB | "
                                f"è€—æ—¶: {duration:.1f}s | "
                                f"é€Ÿåº¦: {speed:.1f}MB/s")
                    return resp.body.etag
                else:
                    logger.warning(
                        f"âš ï¸ åˆ†ç‰‡ #{p_num} ä¸Šä¼ å¤±è´¥ (HTTP {resp.status}, Code: {resp.errorCode}, Msg: {resp.errorMessage})ï¼Œæ­£åœ¨é‡è¯• {i + 1}/{self.max_retries}...")

            except Exception as ex:
                logger.warning(f"âŒ åˆ†ç‰‡ #{p_num} å‘ç”Ÿå¼‚å¸¸: {ex}ï¼Œæ­£åœ¨é‡è¯• {i + 1}/{self.max_retries}...")
            # å¤±è´¥åç¨å¾®ç­‰å¾…ä¸€ä¸‹å†é‡è¯•
            time.sleep(1 * (i + 1))
        raise Exception(f"åˆ†ç‰‡ #{p_num} åœ¨ {self.max_retries} æ¬¡å°è¯•åæœ€ç»ˆå¤±è´¥")

    def _wait_and_collect(self, futures, parts_map):
        """ç­‰å¾…éƒ¨åˆ†ä»»åŠ¡å®Œæˆï¼Œå›æ”¶å†…å­˜ï¼Œæ”¶é›† ETag"""
        done, _ = wait(futures.keys(), return_when=FIRST_COMPLETED)
        for f in done:
            p_num = futures.pop(f)
            try:
                parts_map[p_num] = f.result()
            except Exception as e:
                # è¿™é‡Œæ•è·å¼‚å¸¸æ˜¯ä¸ºäº†ä¸æ‰“æ–­ä¸»å¾ªç¯ï¼Œ
                # ä½†å®é™…ä¸Šå¦‚æœåˆ†ç‰‡å¤±è´¥ï¼Œæœ€ç»ˆåˆå¹¶ä¼šå¤±è´¥ï¼Œæˆ–è€…ä¸Šé¢å·²ç»æŠ›å‡ºäº†
                raise e

    def _fetch_uploaded_parts_map(self, key, uid):
        """è·å–æœåŠ¡ç«¯å·²æœ‰çš„åˆ†ç‰‡ä¿¡æ¯ (PartNum -> ETag)"""
        mapping = {}
        marker = None
        while True:
            resp = self.client.listParts(self.bucket, key, uid, partNumberMarker=marker)
            if resp.status >= 300: break
            for p in resp.body.parts:
                mapping[p.partNumber] = p.etag
            if not resp.body.isTruncated: break
            marker = resp.body.nextPartNumberMarker
        return mapping

    def _complete_upload(self, key, uid):
        """åˆå¹¶åˆ†ç‰‡"""
        logger.info("æµä¼ è¾“ç»“æŸï¼Œæ­£åœ¨è¯·æ±‚åˆå¹¶åˆ†ç‰‡...")

        # ä½¿ç”¨æœ€æ–°çš„ parts_map (åŒ…å«å†å²çš„å’Œæœ¬æ¬¡ä¸Šä¼ çš„)
        # å¦‚æœ _process_stream æˆåŠŸæ‰§è¡Œï¼Œself._final_parts_map åº”è¯¥æœ‰å®Œæ•´æ•°æ®
        # ä¸ºäº†ä¿é™©ï¼Œè¿™é‡Œå¯ä»¥ä½¿ç”¨ _fetch_uploaded_parts_map å†æ¬¡ä»æœåŠ¡ç«¯ç¡®è®¤ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨å†…å­˜ä¸­çš„ map
        # è¿™é‡Œç›´æ¥ä½¿ç”¨å†…å­˜ç´¯ç§¯çš„ mapï¼Œå®ƒæ˜¯æœ€å‡†ç¡®çš„ï¼ˆåŒ…å«æœ¬æ¬¡ä¸Šä¼ ç»“æœï¼‰
        if not hasattr(self, '_final_parts_map') or not self._final_parts_map:
            # å…œåº•ï¼šå¦‚æœå†…å­˜æ²¡æ•°æ®ï¼ˆæ¯”å¦‚æµæ˜¯ç©ºçš„ï¼‰ï¼Œå°è¯•æŸ¥æœåŠ¡ç«¯
            self._final_parts_map = self._fetch_uploaded_parts_map(key, uid)

        if not self._final_parts_map:
            raise Exception("æœªæ‰¾åˆ°ä»»ä½•åˆ†ç‰‡ï¼Œæ— æ³•åˆå¹¶æ–‡ä»¶")

        # æ„é€ åˆå¹¶è¯·æ±‚åˆ—è¡¨ (å¿…é¡»æŒ‰ PartNum æ’åº)
        sorted_parts = [
            CompletePart(partNum=k, etag=v)
            for k, v in sorted(self._final_parts_map.items())
        ]

        resp = self.client.completeMultipartUpload(
            self.bucket, key, uid,
            CompleteMultipartUploadRequest(sorted_parts)
        )
        self._check_error(resp)
        logger.info(f"âœ… ä¸Šä¼ æˆåŠŸ: {key}")

    def _check_error(self, resp):
        if resp.status >= 300:
            raise Exception(f"OBS Error {resp.errorCode}: {resp.errorMessage}")